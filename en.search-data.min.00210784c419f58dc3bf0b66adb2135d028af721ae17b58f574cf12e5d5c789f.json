[{"id":0,"href":"/docs/contributing/devenv/","title":"The development environment","section":"Contributing","content":"In order to test and experiment with OpenPERouter, a containerlab and kind based environment is available.\nTo start it, run\nmake deploy The topology of the environment is as follows:\nWith:\nTwo kind nodes connected to a leaf, running OpenPERouter A spine container Two EVPN enabled leaves, leafA and leafB For each leaf, two hosts connected to a type 5 EVPN (VNI 100 and 200) The kubeconfig file required to interact with the cluster is created under bin/kubeconfig.\nThe leaf the kind cluster is connected to is configured with the following parameters:\nIP: 192.168.11.2 ASN: 64512\nand it is configured to accept BGP session from any peer coming from the network 192.168.11.0/24 with ASN 64514.\nMore details, including the IP addresses of all the nodes involved, can be found on the project readme.\nVeth recreation # The development environment faces a significant issue:\nthe nodes of the topology are containers the interfaces that connect the various node are veth pairs if the network namespace wrapping a veth (or any virtual interface) gets deleted, the veth gets deleted too OpenPERouter works by moving one interface from the host (the kind node) to the pod running inside of it Because of this, when the router pod wrapping the interface gets deleted (instead of returning to the host as it would happen with a real interface).\nTo emulate the behavior of a real system, there is a background script which checks for the deletion of the veths and recreates them.\n"},{"id":1,"href":"/docs/contributing/code-of-conduct/","title":"Code of Conduct","section":"Contributing","content":" Our Pledge # In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.\nOur Standards # Examples of behavior that contributes to creating a positive environment include:\nUsing welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include:\nThe use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others\u0026rsquo; private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities # Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.\nScope # This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.\nEnforcement # Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting fpaoline@redhat.com and rbryant@redhat.com. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.\nProject maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project\u0026rsquo;s leadership.\nAttribution # This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n"},{"id":2,"href":"/docs/concepts/","title":"Concepts","section":"Docs","content":"This section explains the core concepts behind OpenPERouter and how it integrates with your network infrastructure.\nOverview # OpenPERouter transforms Kubernetes nodes into Provider Edge (PE) routers by running FRR in a dedicated network namespace. This enables EVPN (Ethernet VPN) functionality directly on your Kubernetes nodes, eliminating the need for external PE routers.\nNetwork Architecture # Traditional vs. OpenPERouter Architecture # In traditional deployments, Kubernetes nodes connect to Top-of-Rack (ToR) switches via VLANs, and external PE routers handle EVPN tunneling. OpenPERouter moves this PE functionality directly into the Kubernetes nodes.\nKey Components # OpenPERouter consists of three main components:\nRouter Pod: Runs FRR in a dedicated network namespace Controller Pod: Manages network configuration and orchestrates the router setup Node Labeler: Assigns persistent node indices for resource allocation Underlay Connectivity # Fabric Integration # OpenPERouter integrates with your network fabric by establishing BGP sessions with external routers (typically ToR switches).\nNetwork Interface Management # OpenPERouter works by moving the physical network interface connected to the ToR switch into the router\u0026rsquo;s network namespace:\nThis allows the router to establish direct BGP sessions with the fabric and receive routing information.\nVTEP IP Assignment # Each OpenPERouter instance is assigned a unique VTEP (Virtual Tunnel End Point) IP address from a configured CIDR range. This VTEP IP serves as the identifier for the router within the fabric.\nOpenPERouter establishes a BGP session with the fabric, advertising its VTEP IP to other routers. The VPN address family is enabled on this session, allowing the exchange of EVPN routes required for overlay connectivity.\nOverlay Networks (VNIs) # Virtual Network Identifiers # OpenPERouter supports the creation of multiple VNIs (Virtual Network Identifiers), each corresponding to a separate EVPN tunnel. This enables multi-tenancy and network segmentation.\nOpenPERouter supports the creation of L3 VNIs, allowing the extension of a routed domain via an EVPN overlay:\nIt supports the creation of L2 VNIs, where the veth is directly connected to a layer 2 domain\nAnd it also supports a mixed scenario, where an L2 domain also belongs to a broader L3 domain mapped to an L3 overlay\nL3 VNI Components # For each Layer 3 VNI, OpenPERouter automatically creates:\nVeth Pair: Named after the VNI (e.g., host-200@pe-200) for host connectivity BGP Session: Configured over the veth pair connecting the router to the host, to allow BGP connectivity with the host Linux VRF: Isolates the routing space for each VNI within the router\u0026rsquo;s network namespace VXLAN Interface: Handles tunnel encapsulation/decapsulation Route Translation: Converts between BGP routes and EVPN Type 5 routes IP Allocation Strategy # The IP addresses for the veth pair are allocated from the configured localcidr for each VNI:\nRouter side: Always gets the first IP in the CIDR (e.g., 192.169.11.0) Host side: Each node gets a different IP from the CIDR, starting from the second value (e.g., 192.169.11.15) This consistent allocation strategy of the router IP simplifies configuration across all nodes, as any BGP-speaking component on the host can use the same IP address for the router side of the veth pair.\nControl Plane Operations # Route Advertisement (Host → Fabric) # When a BGP-speaking component (like MetalLB) advertises a prefix to OpenPERouter:\nThe host advertises the route with the veth interface IP as the next hop OpenPERouter learns the route via the BGP session OpenPERouter translates the route to an EVPN Type 5 route The EVPN route is advertised to the fabric with the local VTEP as the next hop Route Reception (Fabric → Host) # When EVPN Type 5 routes are received from the fabric:\nOpenPERouter installs the routes in the VRF corresponding to the VNI OpenPERouter translates the EVPN routes to BGP routes The BGP routes are advertised to the host via the veth interface The host\u0026rsquo;s BGP-speaking component learns and installs the routes Data Plane Operations # Egress Traffic Flow # Traffic destined for networks learned via EVPN follows this path:\nHost Routing: Traffic is redirected to the veth interface corresponding to the VNI Encapsulation: OpenPERouter encapsulates the traffic in VXLAN packets with the appropriate VNI Fabric Routing: The fabric routes the VXLAN packets to the destination VTEP Delivery: The destination endpoint instance receives and processes the traffic Ingress Traffic Flow # VXLAN packets received from the fabric are processed as follows:\nDecapsulation: OpenPERouter removes the VXLAN header VRF Routing: Traffic is routed within the VRF corresponding to the VNI Host Delivery: Traffic is forwarded to the host via the veth interface Final Routing: The host routes the traffic to the appropriate destination L2 VNI # For each Layer 2 VNI, OpenPERouter automatically creates:\nVeth Pair: Named after the VNI (e.g., host-200@pe-200) for Layer 2 host connectivity Linux VRF: Optional, isolates the routing space for each VNI within the router\u0026rsquo;s network namespace VXLAN Interface: Handles tunnel encapsulation/decapsulation Host Interface Management # Given the Layer 2 nature of these connections, OpenPERouter supports multiple interface management options:\nAttaching to an existing bridge: If a bridge already exists and is used by other components, OpenPERouter can attach the veth interface to it Creating a new bridge: OpenPERouter can create a bridge and attach the veth interface directly to it. Direct veth usage: With the understanding that the veth interface may disappear if the pod gets restarted, the veth can be used directly to extend an existing Layer 2 domain Automatic Bridge Creation # The automatic bridge creation is useful for those scenarios where an existing layer 2 domain is extended automatically through the veth interface: when the router pod is deleted or restarted, the veth interface is removed (and then recreated upon reconciliation), while the bridge remains intact, making it a good candidate for attaching to an existing layer 2 domain (i.e. setting it as master of a macvlan multus interface).\nData Plane Operations # The following sections describe complete Layer 2 and Layer 3 scenarios for reference:\nEgress Traffic Flow # When Layer 2 traffic arrives at the veth interface and the destination belongs to the same subnet, the traffic is encapsulated and directed to the VTEP where the endpoint with the MAC address corresponding to the destination IP is located.\nIf the destination IP is on a different subnet, the traffic is routed to the L3 domain that the VNI is connected to, and it\u0026rsquo;s routed via the L3 VNI corresponding to the VRF that the veth interface is attached to.\nIngress Traffic Flow # The ingress flow follows the reverse path of the egress flow. For Layer 2 traffic, VXLAN packets are decapsulated and forwarded to the appropriate veth interface. For Layer 3 traffic, packets are routed through the VRF and then forwarded to the host via the veth interface. The process is straightforward and doesn\u0026rsquo;t require additional explanation beyond what has already been covered in the egress flow descriptions.\n"},{"id":3,"href":"/docs/examples/metallb/","title":"MetalLB Integration","section":"Examples","content":"This example demonstrates how to integrate OpenPERouter with MetalLB to advertise LoadBalancer services across the EVPN fabric, enabling external access to Kubernetes services.\nOverview # MetalLB provides load balancing for Kubernetes services by advertising service IPs via BGP. When integrated with OpenPERouter, these BGP routes are automatically converted to EVPN Type 5 routes, making the services reachable across the entire fabric.\nExample Setup # The full example can be found in the project repository and can be deployed by running\nmake demo-metallb This example exposes two different services over two different overlays by configuring two L3 VNIs on the OpenPERouter and peering MetalLB on the sessions corresponding to each VNI.\nRoute Advertisement Flow # Once configured, the integration works as follows:\nService Creation: Kubernetes LoadBalancer service is created with an IP from the pool MetalLB Processing: MetalLB assigns an IP and starts advertising it via BGP BGP Session: MetalLB advertises the route to OpenPERouter through the veth interface EVPN Conversion: OpenPERouter converts the BGP route to EVPN Type 5 route Fabric Distribution: The EVPN route is distributed to all fabric routers External Reachability: External hosts can now reach the service through the fabric Traffic Flow # When external traffic reaches the service:\nExternal Request: External host sends traffic to the service IP Fabric Routing: Fabric routes the traffic to an appropriate VTEP VXLAN Encapsulation: Traffic is encapsulated in VXLAN with the appropriate VNI OpenPERouter Processing: OpenPERouter receives and decapsulates the traffic Service Delivery: Traffic is forwarded to the Kubernetes service endpoint Service Reply: The service reply finds the route learned by FRR-K8s and is routed to the host that sent the request Configuration # L3 VNI Configuration # Configure one L3VNI for each overlay:\napiVersion: openpe.openperouter.github.io/v1alpha1 kind: L3VNI metadata: name: red namespace: openperouter-system spec: asn: 64514 vni: 100 localcidr: 192.169.10.0/24 hostasn: 64515 --- apiVersion: openpe.openperouter.github.io/v1alpha1 kind: L3VNI metadata: name: blue namespace: openperouter-system spec: asn: 64514 vni: 200 localcidr: 192.169.11.0/24 hostasn: 64515 MetalLB BGP Peer Configuration # On the MetalLB side, configure the corresponding BGPPeer resources:\napiVersion: metallb.io/v1beta2 kind: BGPPeer metadata: name: peerred namespace: metallb-system spec: myASN: 64515 peerASN: 64514 peerAddress: 192.169.10.1 --- apiVersion: metallb.io/v1beta2 kind: BGPPeer metadata: name: peerblue namespace: metallb-system spec: myASN: 64515 peerASN: 64514 peerAddress: 192.169.11.1 Note: The peerAddress field specifies the router-side IP address. MetalLB establishes BGP sessions with OpenPERouter through the veth interfaces created for each VNI. Since the router-side IP is consistent across all nodes, you only need one BGPPeer configuration per VNI.\nBGP Advertisement Configuration # MetalLB is configured with two address pools associated with two different namespaces (omitted here for brevity) and configured to advertise each pool to the corresponding BGPPeer:\napiVersion: metallb.io/v1beta1 kind: BGPAdvertisement metadata: name: bgp-advertisement-red namespace: metallb-system spec: ipAddressPools: - address-pool-red peers: - peerred --- apiVersion: metallb.io/v1beta1 kind: BGPAdvertisement metadata: name: bgp-advertisement-blue namespace: metallb-system spec: ipAddressPools: - address-pool-blue peers: - peerblue Return Traffic Configuration # Once the pod behind the service replies, the traffic must find its way back to the host that sent the request.\nTo enable this, the host must learn the routes advertised by the two leaves (leafA and leafB) via BGP. We leverage the integration with frr-k8s to add a configuration that allows incoming routes to be added to the host:\napiVersion: frrk8s.metallb.io/v1beta1 kind: FRRConfiguration metadata: name: receive-all namespace: frr-k8s-system spec: bgp: routers: - asn: 64515 neighbors: - address: 192.169.10.1 asn: 64514 toReceive: allowed: mode: all - address: 192.169.11.1 asn: 64514 toReceive: allowed: mode: all Verification # Check BGP Session Status # Verify that the BGP sessions between MetalLB/FRR-K8s and the OpenPERouter are established:\nkubectl get bgpsessionstates.frrk8s.metallb.io -A Expected output:\nNAMESPACE NAME NODE PEER VRF BGP BFD frr-k8s-system pe-kind-control-plane-94ct2 pe-kind-control-plane 192.169.11.1 Established N/1 frr-k8s-system pe-kind-control-plane-bc9zh pe-kind-control-plane 192.169.10.1 Established N/A frr-k8s-system pe-kind-worker-496lk pe-kind-worker 192.169.11.1 Established N/A frr-k8s-system pe-kind-worker-s74kn pe-kind-worker 192.169.10.1 Established N/A Test Service Connectivity # Test from Red Overlay # First, check the service details:\nkubectl get svc -n red Expected output:\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-service-red LoadBalancer 10.96.161.254 172.30.0.10 80:32732/TCP 2m3s Test connectivity from a host connected to the red overlay:\ndocker exec clab-kind-hostA_red curl 172.30.0.10 Expected output:\n\u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Test from Blue Overlay # Try to access the same service from a host connected to the blue overlay:\ndocker exec clab-kind-hostA_blue curl --max-time 2 --no-progress-meter 172.30.0.10 Expected output:\ncurl: (28) Connection timed out after 2001 milliseconds Expected Behavior: The connection times out because the service is advertised only to the red network, demonstrating the network isolation between overlays.\n"},{"id":4,"href":"/docs/architecture/","title":"Architecture","section":"Docs","content":"This document describes the internal architecture of OpenPERouter and how its components work together to provide EVPN functionality on Kubernetes nodes.\nSystem Overview # OpenPERouter consists of three main components:\nRouter Pod: Runs FRR in a dedicated network namespace Controller Pod: Manages network configuration and orchestrates the router setup Labeler Pod: Assigns persistent node indices for resource allocation Component Architecture # Router Pod # The router pod is the core networking component that provides the actual EVPN functionality.\nThe router pod runs as a Daemonset to allow EVPN connectivity to every node.\nPurpose and Responsibilities # The router pod runs FRR in a dedicated network namespace and is responsible for:\nBGP Sessions: Establishing and maintaining BGP sessions with external routers and host components EVPN Route Processing: Handling EVPN Type 5 route advertisement and reception VXLAN Encapsulation: Managing VXLAN tunnel encapsulation and decapsulation Route Translation: Converting between BGP routes and EVPN routes Network Namespace Management: Operating in an isolated network namespace for security and isolation Container Architecture # The router pod consists of two containers:\nFRR Container: Runs the Free Range Routing daemon and handles all BGP and EVPN operations Reloader Sidecar: Provides an HTTP endpoint that accepts FRR configuration updates and triggers configuration reloads The reloader sidecar container enables dynamic configuration updates without requiring pod restarts, allowing the controller to push new FRR configurations as network conditions change.\nNetwork Configuration Requirements # To enable EVPN functionality, FRR requires specific network interfaces and configurations:\nLinux VRF: Creates isolated Layer 3 routing domains for each VNI Linux Bridge: Connects VXLAN interfaces to the VRF for proper traffic flow VXLAN Interface: Handles tunnel encapsulation/decapsulation with the configured VNI VTEP Loopback: Provides the VTEP IP address for tunnel endpoint identification FRR Configuration # The router pod configures FRR with the following key settings:\nVTEP Advertisement: Advertises the local VTEP IP to the fabric for route reachability EVPN Address Family: Enables EVPN route exchange with external routers Host BGP Sessions: Establishes BGP sessions with components running on the host (L3VNIs) Route Policies: Configures import/export policies for proper route filtering For detailed FRR configuration information, refer to the official FRR documentation.\nController Pod # The controller pod is the orchestration component that manages the router configuration and network setup.\nPurpose and Responsibilities # The controller pod handles all the complex network configuration logic and is responsible for:\nResource Reconciliation: Watches and reconciles OpenPERouter Custom Resources (CRs) Network Interface Management: Moves host interfaces into the router\u0026rsquo;s network namespace VNI Setup: Creates and configures network interfaces for each VNI Configuration Generation: Generates and applies FRR configuration State Management: Maintains the desired state of network configurations Reconciliation Process # The controller follows a specific sequence when reconciling VNI configurations:\nNetwork Interface Creation: For each VNI, creates the required network interfaces (bridge, VRF, VXLAN) for FRR operation L3 Veth Pair Setup: Creates veth pairs to connect VRFs to the host, assigns IPs from the localCIDR, and moves one end to the router\u0026rsquo;s namespace L2 Veth Pair Setup: Creates veth pairs to connect VRFs to the host, enslaves the PERouter side to the bridge corresponding to the L2 domain, eventually creates a bridge on the host, and enslaves the host side to the bridge it just created or to an existing bridge (configurable) Configuration Deployment: Generates the FRR configuration and sends it to the router pod for application When reconciling an Underlay instance, the controller moves the host interface connected to the external router into the router\u0026rsquo;s pod network namespace.\nNode Labeler # The node labeler is a critical component that ensures consistent resource allocation across the cluster.\nPurpose and Responsibilities # The node labeler provides persistent node indexing and is responsible for:\nNode Index Assignment: Assigns a unique, persistent index to each node in the cluster Resource Allocation: Enables deterministic allocation of VTEP IPs and local CIDRs State Persistence: Ensures resource allocations remain consistent across pod restarts and cluster reboots Index Persistence # The node labeler persists the assigned index as a Kubernetes node label, making it available to other components. This ensures:\nConsistency: Each node maintains its assigned index even after pod rescheduling Deterministic Allocation: VTEP IPs and CIDRs are allocated based on the persistent index "},{"id":5,"href":"/docs/examples/calico/","title":"Calico Integration","section":"Examples","content":"This example demonstrates how to integrate OpenPERouter with Calico to advertise the pod network via an L3 EVPN and to allow pod-to-pod traffic via a VXLAN overlay.\nOverview # Calico allows each node to establish a BGP session with a router to advertise the pod network of each node and to allow pod-to-pod traffic to flow. Here we leverage OpenPERouter to provide a seamless integration with the EVPN fabric.\nExample Setup # The full example can be found in the project repository and can be deployed by running deployed by running:\nmake demo-calico This example configures Calico to advertise the pod network to the OpenPERouter and runs a pod on each node. It then shows how the pods are connected via the overlay and reachable from the fabric.\nControl Plane # The Bird instance of Calico running on each node peers with the OpenPERouter, learning routes to the other node\u0026rsquo;s pods and advertising the routes related to the node it\u0026rsquo;s running on. The routes are then translated to EVPN Type 5 routes to the fabric, including the other OpenPERouter instance.\nData Path # When a pod tries to reach a pod running on another node, the traffic lands on the host, then in the OpenPERouter via the veth interface, and then gets encapsulated as a VXLAN packet toward the other node, where it\u0026rsquo;s decapsulated and sent to the node.\nConfiguration # OpenPERouter Configuration # The OpenPERouter configuration is simple: one underlay and one L3VNI configured to peer with the host:\napiVersion: openpe.openperouter.github.io/v1alpha1 kind: L3VNI metadata: name: red namespace: openperouter-system spec: asn: 64514 vni: 100 localcidr: 192.169.10.0/24 hostasn: 64515 --- apiVersion: openpe.openperouter.github.io/v1alpha1 kind: Underlay metadata: name: underlay namespace: openperouter-system spec: asn: 64514 vtepcidr: 100.65.0.0/24 nics: - toswitch neighbors: - asn: 64512 address: 192.168.11.2 Calico Configuration # Calico is configured to establish a session with the same router IP on every node:\napiVersion: projectcalico.org/v3 kind: BGPPeer metadata: name: openpe spec: peerIP: 192.169.10.1 asNumber: 64514 numAllowedLocalASNumbers: 5 --- apiVersion: projectcalico.org/v3 kind: BGPConfiguration metadata: name: default spec: nodeToNodeMeshEnabled: false logSeverityScreen: Info asNumber: 64515 serviceClusterIPs: - cidr: 10.96.0.0/12 serviceExternalIPs: - cidr: 104.244.42.129/32 - cidr: 172.217.3.0/24 listenPort: 179 bindMode: NodeIP Additionally, a node status is created to validate the status of the BGP session:\napiVersion: projectcalico.org/v3 kind: CalicoNodeStatus metadata: name: status spec: classes: - BGP node: pe-kind-worker updatePeriodSeconds: 10 Verification # Check BGP Session Status # Verify the status of the BGP session between Calico and OpenPERouter:\nkubectl get caliconodestatuses.projectcalico.org status -o yaml Expected output:\napiVersion: projectcalico.org/v3 kind: CalicoNodeStatus metadata: name: status spec: classes: - BGP node: pe-kind-worker updatePeriodSeconds: 10 status: agent: birdV4: {} birdV6: {} bgp: numberEstablishedV4: 1 numberEstablishedV6: 0 numberNotEstablishedV4: 0 numberNotEstablishedV6: 0 peersV4: - peerIP: 192.169.10.1 since: \u0026#34;15:55:08\u0026#34; state: Established type: GlobalPeer Test Pod-to-Pod Connectivity # First, check the two pods running on different nodes:\nkubectl get pods -o wide Expected output:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES agnhost-daemonset-97jjq 1/1 Running 0 33m 10.244.97.8 pe-kind-control-plane \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; agnhost-daemonset-zj6sg 1/1 Running 0 33m 10.244.1.195 pe-kind-worker \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Test connectivity from one pod to the other:\nkubectl exec agnhost-daemonset-97jjq -- curl --no-progress-meter 10.244.1.195:8090 Expected output:\nNOW: 2025-07-01 16:29:16.788070872 +0000 UTC m=+2044.193991270 Success: The pods can communicate with each other!\nTest External Access # Test accessing a pod from a host on the red overlay. This demonstrates that the pod IP is routable and exposed on the EVPN overlay:\ndocker exec clab-kind-hostA_red curl --no-progress-meter 10.244.1.195:8090 Expected output:\nNOW: 2025-07-01 16:30:55.312723398 +0000 UTC m=+2142.718643795 Verify VXLAN Encapsulation # To verify that traffic is actually running over VXLAN, we can monitor the traffic while pinging between pods:\nkubectl exec agnhost-daemonset-97jjq -- ping 10.244.1.195 Expected output:\nPING 10.244.1.195 (10.244.1.195): 56 data bytes 64 bytes from 10.244.1.195: seq=0 ttl=60 time=0.161 ms 64 bytes from 10.244.1.195: seq=1 ttl=60 time=0.135 ms 64 bytes from 10.244.1.195: seq=2 ttl=60 time=0.098 ms 64 bytes from 10.244.1.195: seq=3 ttl=60 time=0.114 ms The traffic flow observed in the router shows VXLAN encapsulation:\n6:33:38.392401 pe-100 In IP 10.244.97.8 \u0026gt; 10.244.1.195: ICMP echo request, id 10752, seq 0, length 64 16:33:38.392411 br-pe-100 Out IP 10.244.97.8 \u0026gt; 10.244.1.195: ICMP echo request, id 10752, seq 0, length 64 16:33:38.392413 vni100 Out IP 10.244.97.8 \u0026gt; 10.244.1.195: ICMP echo request, id 10752, seq 0, length 64 16:33:38.392419 toswitch Out IP 100.65.0.0.47720 \u0026gt; 100.65.0.1.4789: VXLAN, flags [I] (0x08), vni 100 IP 10.244.97.8 \u0026gt; 10.244.1.195: ICMP echo request, id 10752, seq 0, length 64 16:33:38.392488 toswitch In IP 100.65.0.1.47720 \u0026gt; 100.65.0.0.4789: VXLAN, flags [I] (0x08), vni 100 IP 10.244.1.195 \u0026gt; 10.244.97.8: ICMP echo reply, id 10752, seq 0, length 64 16:33:38.392494 vni100 In IP 10.244.1.195 \u0026gt; 10.244.97.8: ICMP echo reply, id 10752, seq 0, length 64 16:33:38.392495 br-pe-100 In IP 10.244.1.195 \u0026gt; 10.244.97.8: ICMP echo reply, id 10752, seq 0, length 64 16:33:38.392499 pe-100 Out IP 10.244.1.195 \u0026gt; 10.244.97.8: ICMP echo reply, id 10752, seq 0, length 64 Traffic Flow Analysis:\nIncoming: The ICMP packet comes from the host through the veth interface Encapsulation: It leaves through the VXLAN interface and gets encapsulated as a VXLAN packet toward the VTEP of the other node Return: The reply follows the same path in reverse VXLAN Header: Notice the VXLAN encapsulation with VNI 100 and the outer IP addresses (100.65.0.0/100.65.0.1) "},{"id":6,"href":"/docs/installation/","title":"Installation","section":"Docs","content":"This guide covers the installation of OpenPERouter using different deployment methods.\nPrerequisites # Before installing OpenPERouter, ensure you have:\nA Kubernetes cluster (v1.20 or later) kubectl configured to communicate with your cluster Cluster administrator privileges (for creating namespaces and CRDs) Network interfaces configured for BGP peering with external routers For OpenShift: oc CLI tool and cluster admin access Installation Methods # OpenPERouter offers several deployment methods to suit different environments and preferences:\nMethod 1: All-in-One Manifests (Quick Start) # The simplest way to install OpenPERouter is using the all-in-one manifests. This method is ideal for testing and development environments.\nStandard Installation (containerd) # kubectl apply -f https://raw.githubusercontent.com/openperouter/openperouter/v0.0.3/config/all-in-one/openpe.yaml CRI-O Variant # If your cluster uses CRI-O as the container runtime:\nkubectl apply -f https://raw.githubusercontent.com/openperouter/openperouter/v0.0.3/config/all-in-one/crio.yaml Method 2: Kustomize Installation # Kustomize provides more flexibility for customizing the deployment. This method is recommended for production environments.\nDefault Configuration # Create a kustomization.yaml file:\nnamespace: openperouter-system resources: - github.com/openperouter/openperouter/config/default?ref=v0.0.3 Then apply it:\nkubectl apply -k . CRI-O Variant with Kustomize # namespace: openperouter-system resources: - github.com/openperouter/openperouter/config/crio?ref=v0.0.3 Method 3: Helm Installation # Helm provides the most flexibility for configuration and is recommended for production deployments.\nAdd the Helm Repository # helm repo add openperouter https://openperouter.github.io/openperouter helm repo update Install OpenPERouter # helm install openperouter openperouter/openperouter Customize Installation # You can customize the installation by creating a values file:\n# values.yaml openperouter: logLevel: \u0026#34;info\u0026#34; cri: \u0026#34;containerd\u0026#34; frr: image: repository: \u0026#34;quay.io/frrouting/frr\u0026#34; tag: \u0026#34;10.2.1\u0026#34; Then install with custom values:\nhelm install openperouter openperouter/openperouter -f values.yaml Platform-Specific Instructions # OpenShift # When running on OpenShift, additional Security Context Constraints (SCCs) must be configured:\noc adm policy add-scc-to-user privileged -n openperouter-system -z controller oc adm policy add-scc-to-user privileged -n openperouter-system -z router Verification # After installation, verify that all components are running correctly:\nkubectl get pods -n openperouter-system You should see pods for:\nopenperouter-controller-* (controller daemonset) openperouter-router-* (router daemonset) openperouter-nodemarker-* (node labeler deployment) Next Steps # After successful installation:\nConfigure the underlay connection to your external router Set up VNI configurations for your EVPN overlays Test the integration with BGP-speaking components For troubleshooting, check the contributing guide for development environment setup and debugging information.\n"},{"id":7,"href":"/docs/examples/layer2/","title":"Layer 2 Integration","section":"Examples","content":"This example demonstrates how to integrate OpenPERouter with Multus to have a pod\u0026rsquo;s secondary interface connected to a layer 2 overlay.\nOverview # A layer 2 VNI is created, exposing a layer 2 domain on the host. On each node, a pod is created with a macvlan interface enslaved to that domain via a Linux bridge.\nExample Setup # The full example can be found in the project repository and can be deployed by running:\nmake demo-l2 The example configures both an L2 VNI and an L3 VNI. The L2 VNI belongs to the L3 VNI\u0026rsquo;s routing domain. Pods are running on two separate nodes and connected via the overlay. Additionally, the pods are able to reach the broader L3 domain.\nPod-to-Pod Connectivity # The two pods are on the same L2 subnet. When a pod tries to reach the other, no routing is involved and the L2 traffic gets encapsulated in the corresponding L2 VNI.\nPod-to-Host Connectivity # When a pod needs to reach a host belonging to the L3 domain the L2 VNI belongs to, the traffic is routed in the VRF of the OpenPERouter the pod is connected to, and routed according to the Type 5 routes.\nConfiguration # OpenPERouter Configuration # One L3 VNI corresponding to the routing domain, one L2 VNI with the same VRF as the L3 VNI:\napiVersion: openpe.openperouter.github.io/v1alpha1 kind: L3VNI metadata: name: red namespace: openperouter-system spec: asn: 64514 vni: 100 localcidr: 192.169.10.0/24 hostasn: 64515 --- apiVersion: openpe.openperouter.github.io/v1alpha1 kind: L2VNI metadata: name: layer2 namespace: openperouter-system spec: vni: 110 vrf: red hostmaster: type: bridge autocreate: true l2gatewayip: 192.170.1.0/24 Configuration Notes:\nl2gatewayip field: Allows each pod to have the same default gateway and be able to send traffic to the outer L3 domain hostmaster.autocreate: Instructs OpenPERouter to create a bridge local to the node that can be used to access the L2 domain Network Attachment Definition # apiVersion: \u0026#34;k8s.cni.cncf.io/v1\u0026#34; kind: NetworkAttachmentDefinition metadata: name: macvlan-conf spec: config: \u0026#39;{ \u0026#34;cniVersion\u0026#34;: \u0026#34;0.3.0\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;macvlan\u0026#34;, \u0026#34;master\u0026#34;: \u0026#34;br-hs-110\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;static\u0026#34;, \u0026#34;routes\u0026#34;: [ { \u0026#34;dst\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34;, \u0026#34;gw\u0026#34;: \u0026#34;192.170.1.0\u0026#34; } ] } }\u0026#39; A network attachment definition creating a macvlan interface with:\nThe bridge created by the OpenPERouter as master The default gateway IP assigned to each OpenPERouter instance on every node Workload Configuration # Two pods, each running on a different node, with a Multus secondary network corresponding to the network attachment definition we just created:\napiVersion: v1 kind: Pod metadata: name: second annotations: k8s.v1.cni.cncf.io/networks: \u0026#39;[{ \u0026#34;name\u0026#34;: \u0026#34;macvlan-conf\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;ips\u0026#34;: [\u0026#34;192.170.1.4/24\u0026#34;] }]\u0026#39; spec: nodeSelector: kubernetes.io/hostname: pe-kind-worker containers: - name: agnhost image: k8s.gcr.io/e2e-test-images/agnhost:2.45 command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;ip r del default dev eth0 \u0026amp;\u0026amp; /agnhost netexec --http-port=8090\u0026#34;] ports: - containerPort: 8090 name: http securityContext: capabilities: add: [\u0026#34;NET_ADMIN\u0026#34;] Note: We remove the default gateway so the secondary interface default gateway is the only active one.\nValidation # Pod L2-to-L2 Connectivity # We can check that the first pod is able to curl the second, and the client IP is the one corresponding to the secondary interface:\nkubectl exec -it first -- curl 192.170.1.4:8090/clientip Expected output:\n192.170.1.3:44564 We can monitor traffic in the OpenPERouter\u0026rsquo;s namespace to validate that the traffic is actually going through the overlay:\n17:56:45.152547 pe-110 P IP 192.170.1.3 \u0026gt; 192.170.1.4: ICMP echo request, id 12544, seq 0, length 64 17:56:45.152563 vni110 Out IP 192.170.1.3 \u0026gt; 192.170.1.4: ICMP echo request, id 12544, seq 0, length 64 17:56:45.152578 toswitch Out IP 100.65.0.0.50691 \u0026gt; 100.65.0.1.4789: VXLAN, flags [I] (0x08), vni 110 IP 192.170.1.3 \u0026gt; 192.170.1.4: ICMP echo request, id 12544, seq 0, length 64 17:56:45.152627 toswitch In IP 100.65.0.1.50691 \u0026gt; 100.65.0.0.4789: VXLAN, flags [I] (0x08), vni 110 IP 192.170.1.4 \u0026gt; 192.170.1.3: ICMP echo reply, id 12544, seq 0, length 64 17:56:45.152634 vni110 P IP 192.170.1.4 \u0026gt; 192.170.1.3: ICMP echo reply, id 12544, seq 0, length 64 17:56:45.152637 pe-110 Out IP 192.170.1.4 \u0026gt; 192.170.1.3: ICMP echo reply, id 12544, seq 0, length 64 Traffic Flow Analysis: The traffic comes through the veth corresponding to the pod\u0026rsquo;s secondary interface, then it\u0026rsquo;s sent out directly via the VXLAN interface and encapsulated.\nPod-to-External L3 Connectivity # With 192.168.20.2 being the IP of the host on the red network:\nkubectl exec -it first -- curl 192.168.20.2:8090/clientip Expected output:\n192.170.1.3:37144 The pod is able to reach a host on the layer 3 network and the IP is preserved.\nWhen monitoring traffic:\n18:01:50.406263 pe-110 P IP 192.170.1.3 \u0026gt; 192.168.20.2: ICMP echo request, id 21760, seq 0, length 64 18:01:50.406267 br-pe-110 In IP 192.170.1.3 \u0026gt; 192.168.20.2: ICMP echo request, id 21760, seq 0, length 64 18:01:50.406284 br-pe-100 Out IP 192.170.1.3 \u0026gt; 192.168.20.2: ICMP echo request, id 21760, seq 0, length 64 18:01:50.406287 vni100 Out IP 192.170.1.3 \u0026gt; 192.168.20.2: ICMP echo request, id 21760, seq 0, length 64 18:01:50.406295 toswitch Out IP 100.65.0.0.39370 \u0026gt; 100.64.0.1.4789: VXLAN, flags [I] (0x08), vni 100 IP 192.170.1.3 \u0026gt; 192.168.20.2: ICMP echo request, id 21760, seq 0, length 64 Traffic Flow Analysis: We see that the packet comes from the veth interface towards the bridge associated with VNI 110 (the default gateway), then is routed to the bridge corresponding to the layer 3 domain and finally encapsulated.\n"},{"id":8,"href":"/docs/configuration/","title":"Configuration","section":"Docs","content":"OpenPERouter requires two main configuration components: the Underlay configuration for external router connectivity and VNI configurations for EVPN overlays.\nAll Custom Resources (CRs) must be created in the same namespace where OpenPERouter is deployed (typically openperouter-system).\nUnderlay Configuration # The underlay configuration establishes BGP sessions with external routers (typically Top-of-Rack switches) and defines the VTEP IP allocation strategy.\nBasic Underlay Configuration # apiVersion: openpe.openperouter.github.io/v1alpha1 kind: Underlay metadata: name: underlay namespace: openperouter-system spec: asn: 64514 vtepcidr: 100.65.0.0/24 nics: - toswitch neighbors: - asn: 64512 address: 192.168.11.2 Configuration Fields # Field Type Description Required asn integer Local ASN for BGP sessions Yes vtepcidr string CIDR block for VTEP IP allocation Yes nics array List of network interface names to move to router namespace Yes neighbors array List of BGP neighbors to peer with Yes VTEP IP Allocation # The vtepcidr field defines the IP range used for VTEP (Virtual Tunnel End Point) addresses. OpenPERouter automatically assigns a unique VTEP IP to each node from this range. For example, with 100.65.0.0/24:\nNode 1: 100.65.0.1 Node 2: 100.65.0.2 Node 3: 100.65.0.3 etc. Alternative: Multus Network for Top of Rack Connectivity # Instead of declaring physical network interfaces in the underlay configuration, you can use Multus networks to provide connectivity to top of rack switches. In this case, the nics field in the underlay configuration can be omitted.\nWhen using this approach, ensure that the router pods are configured with the appropriate Multus network annotation to connect to your top of rack switches.\nUsing Helm Values # You can specify the Multus network annotation using Helm values:\n# values.yaml openperouter: multusNetworkAnnotation: \u0026#34;macvlan-conf\u0026#34; Or when installing with Helm:\nhelm install openperouter ./charts/openperouter \\ --set openperouter.multusNetworkAnnotation=\u0026#34;macvlan-conf\u0026#34; This will add the annotation k8s.v1.cni.cncf.io/networks: macvlan-conf to the router pods.\nUsing Kustomize # Alternatively, you can use kustomize to add the annotation to the router pod:\n# kustomization.yaml patches: - target: kind: DaemonSet name: router patch: |- - op: add path: /spec/template/metadata/annotations value: k8s.v1.cni.cncf.io/networks: macvlan-conf L3 VNI Configuration # L3 VNI (Virtual Network Identifier) configurations define EVPN L3 overlays. Each L3VNI creates a separate routing domain and BGP session with the host.\nBasic L3VNI Configuration # apiVersion: openpe.openperouter.github.io/v1alpha1 kind: L3VNI metadata: name: blue namespace: openperouter-system spec: asn: 64514 vni: 200 localcidr: 192.169.11.0/24 hostasn: 64515 Configuration Fields # Field Type Description Required asn integer Router ASN for BGP session with host Yes vni integer Virtual Network Identifier (1-16777215) Yes localcidr string CIDR for veth pair IP allocation Yes hostasn integer Host ASN for BGP session Yes Multiple VNIs Example # You can create multiple VNIs for different network segments:\n# Production VNI apiVersion: openpe.openperouter.github.io/v1alpha1 kind: L3VNI metadata: name: signal namespace: openperouter-system spec: asn: 64514 vni: 100 localcidr: 192.168.10.0/24 hostasn: 64515 --- # Development VNI apiVersion: openpe.openperouter.github.io/v1alpha1 kind: L3VNI metadata: name: oam namespace: openperouter-system spec: asn: 64514 vni: 200 localcidr: 192.168.20.0/24 hostasn: 64515 What Happens During Reconciliation # When you create or update VNI configurations, OpenPERouter automatically:\nCreates Network Interfaces: Sets up VXLAN interface and Linux VRF named after the VNI Establishes Connectivity: Creates veth pair and moves one end to the router\u0026rsquo;s namespace Assigns IP Addresses: Allocates IPs from the localcidr range: Router side: First IP in the CIDR (e.g., 192.169.11.1) Host side: Each node gets a free IP in the CIDR, starting from the second (e.g., 192.169.11.15) Creates BGP Session: Opens BGP session between router and host using the specified ASNs L2VNI Configuration # L2VNIs provide Layer 2 connectivity across nodes using EVPN tunnels. Unlike L3VNIs, L2VNIs extend Layer 2 domains rather than routing domains.\nConfiguration Fields # Field Type Description Required vni integer Virtual Network Identifier for the EVPN tunnel Yes vrf string Name of the VRF to associate with this L2VNI Yes hostmaster.type string Type of host interface management (bridge or direct) Yes hostmaster.autocreate boolean Whether to automatically create a bridge if type is bridge No hostmaster.bridgeName string Name of the bridge to attach to (if not auto-creating) No L2VNI Example # apiVersion: openpe.openperouter.github.io/v1alpha1 kind: L2VNI metadata: name: l2red namespace: openperouter-system spec: vni: 210 vrf: red hostmaster: type: bridge autocreate: true What Happens During Reconciliation # When you create or update VNI configurations, OpenPERouter automatically:\nCreates Network Interfaces: Sets up VXLAN interface and Linux VRF named after the VNI Establishes Connectivity: Creates veth pair and moves one end to the router\u0026rsquo;s namespace Enslaves the veth: the veth is connected to the bridge corresponding to the l2 domain Optionally creates a bridge on the host: if hostmaster.autocreate is set to true Optionally connects the host veth to the bridge on the host: if hostmaster.autocreate is set to true or name is set API Reference # For detailed information about all available configuration fields, validation rules, and API specifications, see the API Reference documentation.\n"},{"id":9,"href":"/docs/examples/","title":"Examples","section":"Docs","content":"This section provides practical examples of integrating OpenPERouter with various BGP-speaking components commonly used in Kubernetes environments.\nOverview # OpenPERouter behaves exactly like a physical Provider Edge (PE) router, enabling seamless integration with any BGP-speaking component. This router-like behavior ensures that integration is straightforward and follows standard BGP peering practices.\nPrerequisites # All examples in this section assume you have:\nOpenPERouter installed and configured (see Installation) A development environment with two L3 VNIs available from the fabric Basic understanding of BGP and EVPN concepts Development Environment Setup # The examples use a development environment with the following topology:\nThis environment provides:\nTwo L3 VNIs (100 and 200) configured in the fabric Leaf switches (leafA and leafB) with BGP peering A kind cluster for testing OpenPERouter integration Base OpenPERouter Configuration # Before running any integration examples, you need to configure OpenPERouter with the appropriate underlay and VNI settings.\nUnderlay Configuration # Configure the underlay to peer with the kind-leaf node:\napiVersion: openpe.openperouter.github.io/v1alpha1 kind: Underlay metadata: name: underlay namespace: openperouter-system spec: asn: 64514 vtepcidr: 100.65.0.0/24 nics: - toswitch neighbors: - asn: 64512 address: 192.168.11.2 Configuration Details:\nASN: 64514 (OpenPERouter\u0026rsquo;s ASN) VTEP CIDR: 100.65.0.0/24 (VTEP IP allocation range) Interface: toswitch (network interface to the fabric) Neighbor: 192.168.11.2 with ASN 64512 (kind-leaf node) VNI Configurations # Create two VNIs that match the fabric configuration:\n# Red VNI (VNI 100) apiVersion: openpe.openperouter.github.io/v1alpha1 kind: VNI metadata: name: red namespace: openperouter-system spec: asn: 64514 vni: 100 localcidr: 192.169.10.0/24 hostasn: 64515 --- # Blue VNI (VNI 200) apiVersion: openpe.openperouter.github.io/v1alpha1 kind: VNI metadata: name: blue namespace: openperouter-system spec: asn: 64514 vni: 200 localcidr: 192.169.11.0/24 hostasn: 64515 VNI Details:\nRed VNI: VNI 100 with CIDR 192.169.10.0/24 Blue VNI: VNI 200 with CIDR 192.169.11.0/24 Host ASN: 64515 (for BGP sessions with host components) Available Examples # MetalLB Integration # Learn how to integrate OpenPERouter with MetalLB to advertise LoadBalancer services across the EVPN fabric.\nKey Features:\nLoadBalancer service advertisement EVPN Type 5 route generation Cross-fabric service reachability View MetalLB Integration Example →\n"},{"id":10,"href":"/docs/api-reference/","title":"API Reference","section":"Docs","content":" API Reference # Packages # openpe.openperouter.github.io/v1alpha1 openpe.openperouter.github.io/v1alpha1 # Package v1alpha1 contains API Schema definitions for the openpe v1alpha1 API group.\nResource Types # L2VNI L3VNI Underlay HostMaster # Appears in:\nL2VNISpec Field Description Default Validation name string Name of the host interface. Must match VRF name validation if set. MaxLength: 15 Pattern: ^[a-zA-Z][a-zA-Z0-9_-]*$ type string Type of the host interface. Currently only \u0026ldquo;bridge\u0026rdquo; is supported. Enum: [bridge] autocreate boolean If true, the interface will be created automatically if not present.The name of the bridge is of the form br-hs-. false L2VNI # L2VNI represents a VXLan VNI to receive EVPN type 2 routes from.\nField Description Default Validation apiVersion string openpe.openperouter.github.io/v1alpha1 kind string L2VNI metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. spec L2VNISpec status L2VNIStatus L2VNISpec # L2VNISpec defines the desired state of VNI.\nAppears in:\nL2VNI Field Description Default Validation vrf string VRF is the name of the linux VRF to be used inside the PERouter namespace.The field is optional, if not set it the name of the VNI instance will be used. MaxLength: 15 Pattern: ^[a-zA-Z][a-zA-Z0-9_-]*$ vni integer VNI is the VXLan VNI to be used Maximum: 4.294967295e+09 Minimum: 0 vxlanport integer VXLanPort is the port to be used for VXLan encapsulation. 4789 hostmaster HostMaster HostMaster is the interface on the host the veth should be enslaved to.If not set, the host veth will not be enslaved to any interface and it must beenslaved manually (or by some other means). This is useful if another controlleris leveraging the host interface for the VNI. l2gatewayip string L2GatewayIP is the IP address to be used for the L2 gateway. When this is set, thebridge the veths are enslaved to will be configured with this IP address, effectivelyacting as a distributed gateway for the VNI. L2VNIStatus # VNIStatus defines the observed state of VNI.\nAppears in:\nL2VNI L3VNI # L3VNI represents a VXLan L3VNI to receive EVPN type 5 routes from.\nField Description Default Validation apiVersion string openpe.openperouter.github.io/v1alpha1 kind string L3VNI metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. spec L3VNISpec status L3VNIStatus L3VNISpec # L3VNISpec defines the desired state of VNI.\nAppears in:\nL3VNI Field Description Default Validation asn integer ASN is the local AS number to use to establish a BGP session withthe default namespace. Maximum: 4.294967295e+09 Minimum: 1 vrf string VRF is the name of the linux VRF to be used inside the PERouter namespace.The field is optional, if not set it the name of the VNI instance will be used. MaxLength: 15 Pattern: ^[a-zA-Z][a-zA-Z0-9_-]*$ hostasn integer ASN is the expected AS number for a BGP speaking component running inthe default network namespace. If not set, the ASN field is going to be used. Maximum: 4.294967295e+09 Minimum: 0 vni integer VNI is the VXLan VNI to be used Maximum: 4.294967295e+09 Minimum: 0 localcidr LocalCIDRConfig LocalCIDR is the CIDR configuration for the veth pairto connect with the default namespace. The interface underthe PERouter side is going to use the first IP of the cidr on all the nodes.At least one of IPv4 or IPv6 must be provided. vxlanport integer VXLanPort is the port to be used for VXLan encapsulation. 4789 L3VNIStatus # L3VNIStatus defines the observed state of L3VNI.\nAppears in:\nL3VNI LocalCIDRConfig # Appears in:\nL3VNISpec Field Description Default Validation ipv4 string IPv4 is the IPv4 CIDR to be used for the veth pairto connect with the default namespace. The interface underthe PERouter side is going to use the first IP of the cidr on all the nodes. ipv6 string IPv6 is the IPv6 CIDR to be used for the veth pairto connect with the default namespace. The interface underthe PERouter side is going to use the first IP of the cidr on all the nodes. Neighbor # Neighbor represents a BGP Neighbor we want FRR to connect to.\nAppears in:\nUnderlaySpec Field Description Default Validation asn integer ASN is the AS number to use for the local end of the session. Maximum: 4.294967295e+09 Minimum: 1 address string Address is the IP address to establish the session with. port integer Port is the port to dial when establishing the session.Defaults to 179. Maximum: 16384 Minimum: 0 password string Password to be used for establishing the BGP session.Password and PasswordSecret are mutually exclusive. passwordSecret string PasswordSecret is name of the authentication secret for the neighbor.the secret must be of type \u0026ldquo;kubernetes.io/basic-auth\u0026rdquo;, and created in thesame namespace as the perouter daemon. The password is stored in thesecret as the key \u0026ldquo;password\u0026rdquo;.Password and PasswordSecret are mutually exclusive. holdTime Duration HoldTime is the requested BGP hold time, per RFC4271.Defaults to 180s. keepaliveTime Duration KeepaliveTime is the requested BGP keepalive time, per RFC4271.Defaults to 60s. connectTime Duration Requested BGP connect time, controls how long BGP waits between connection attempts to a neighbor. ebgpMultiHop boolean EBGPMultiHop indicates if the BGPPeer is multi-hops away. Underlay # Underlay is the Schema for the underlays API.\nField Description Default Validation apiVersion string openpe.openperouter.github.io/v1alpha1 kind string Underlay metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. spec UnderlaySpec status UnderlayStatus UnderlaySpec # UnderlaySpec defines the desired state of Underlay.\nAppears in:\nUnderlay Field Description Default Validation asn integer ASN is the local AS number to use for the session with the TOR switch. Maximum: 4.294967295e+09 Minimum: 1 vtepcidr string VTEPCIDR is CIDR to be used to assign IPs to the local VTEP on each node. neighbors Neighbor array Neighbors is the list of external neighbors to peer with. MinItems: 1 nics string array Nics is the list of physical nics to move under the PERouter namespace to connectto external routers. This field is optional when using Multus networks for TOR connectivity. UnderlayStatus # UnderlayStatus defines the observed state of Underlay.\nAppears in:\nUnderlay "},{"id":11,"href":"/docs/contributing/","title":"Contributing","section":"Docs","content":"We would love to hear from you! Here are some places you can find us.\nIssue Tracker # Use the GitHub issue tracker to file bugs and features request.\nContributing # Contributions are more than welcome! Here\u0026rsquo;s some information to get you started.\nCode of Conduct # This project is released with a Contributor Code of Conduct. By participating in this project you agree to abide by its terms.\nCode changes # Before you make significant code changes, please consider opening a pull request with a proposed design in the design/ directory. That should reduce the amount of time required for code review. If you don\u0026rsquo;t have a full design proposal ready, feel free to open an issue to discuss what you would like to do.\nAll submissions require review. We use GitHub pull requests for this purpose. Consult GitHub Help for more information on using pull requests.\nCertificate of Origin # By contributing to this project you agree to the Developer Certificate of Origin (DCO). This document was created by the Linux Kernel community and is a simple statement that you, as a contributor, have the legal right to make the contribution. See the DCO file for details.\nCode organization # The OpenPERouter codebase is organized to separate core functionality, supporting libraries, deployment resources, and documentation. Here is an overview of the most relevant directories:\ncmd/ – Contains the main entry points for the OpenPERouter binaries. Please check the architecture documentation for an overview of the binaries and their responsibilities. api/ – Defines the Kubernetes Custom Resource Definitions (CRDs) and API types used by OpenPERouter. internal/ – Contains internal packages that implement the core logic of OpenPERouter. This directory is not intended for use outside the project and is structured as follows: controller/ – Implements the controllers logic, including reconciliation loops and resource management for OpenPERouter components. conversion/ – Handles conversion logic between different API versions or resource representations. frr/ – Contains code related to FRRouting (FRR) integration, such as configuration generation and management. frrconfig/ – Manages FRR configuration files and templates used by OpenPERouter. hostnetwork/ – Provides utilities and logic for managing host networking aspects required by FRR to make EVPN work. ipam/ – Implements IP Address Management (IPAM) logic, including allocation and tracking of IP addresses. operator/ – Contains the Kubernetes operator for OpenPERouter, including its main code, API, and configuration. charts/ – Helm charts for deploying OpenPERouter and its components. e2etests/ – End-to-end test suite for validating OpenPERouter functionality. This structure helps keep the project modular, maintainable, and easy to navigate for contributors and users alike.\nIn addition to code, there\u0026rsquo;s deployment configuration and documentation:\nHelm charts: The charts/ directory contains Helm charts for deploying OpenPERouter and its components. These charts provide a convenient way to install, upgrade, and manage OpenPERouter in Kubernetes environments. Refer to the README in the charts/ directory for usage instructions and configuration options.\nConfiguration: Deployment and runtime configuration files are located in the operator/config/ directory. This includes manifests for Kubernetes resources, sample CRs, and other configuration templates. Review these files to understand how to customize OpenPERouter for your environment.\nWebsite and Documentation: The website/ directory contains the source for the OpenPERouter documentation site. Contributions to documentation, guides, and tutorials are welcome. To propose changes, edit the relevant Markdown files and submit a pull request.\nBuilding and running the code # Start by fetching the OpenPERouter repository, with git clone https://github.com/openperouter/openperouter.\nFrom there, you can build the docker image locally by calling\nmake docker-build A comprehensive test environment can be deployed locally by using\nmake deploy Check the dev environment documentation for more details.\nAfter deploying the ContainerLab environment together with Kind, the kubeconfig will be available at bin/kubeconfig.\nRunning the tests locally # You can run unit tests locally with:\nmake test To run the end-to-end (e2e) tests, first ensure that the local development environment is running (see above), then execute:\nmake e2etest This will run the e2e test suite against your local environment.\nCommit Messages # The following are our commit message guidelines:\nLine wrap the body at 72 characters For a more complete discussion of good git commit message practices, see https://chris.beams.io/posts/git-commit/. Extending the end to end test suite # When adding a new feature, or modifying a current one, consider adding a new test to the test suite located in /e2etest. Each feature should come with enough unit test / end to end coverage to make us confident of the change.\n"}]